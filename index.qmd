---
format: revealjs
title: '"La Muerte del Aprendizaje de Máquina"'
subtitle: 'o "cómo la teoría puede mejorar a las máquinas"'
author: George G. Vega Yon, Ph.D.
institute: 
  - University of Utah, US
  - Booz Allen Hamilton, US
slide-level: 2
slide-number: true
embed-resources: true
bibliography: references.bib
footer: '<text style="font-size: 100%">La Muerte del Aprendizaje de Máquina -- <a href="https://ggv.cl/slides/udd-ene2024">https://ggv.cl/slides/udd-ene2024</a></text>'
---

## {background-color="black"} 

Pueden descargar la presentación en https://ggv.cl/slides/udd-ene2024

## Sobre mi

- Profesor de Investigación en Epidemiología de la Universidad de Utah.

- Lead Scientist en la consultora Booz Allen Hamilton.

- Doctor en Bioestadística de la Universidad del Sur de California (USC).

- Magíster en Ciencias Sociales del Instituto Tecnológico de California (Caltech).

- Magíster en Economía y Poíticas Públicas de la Universidad Adolfo Ibáñez (UAI).


# Parte I: La Revolucion de la IA {background-color="black"}

## Definición


> "[L]a capacidad de un sistema para interpretar correctamente datos externos, y así aprender y emplear esos conocimientos para lograr tareas y metas concretas a través de la adaptación flexible."
>
> -- @kaplanSiriSiriMy2019



## AI, Machine Learning, y Estadística {.smaller}

::: {.columns}

::: {.column width="50%"}

::: {.fragment}
- AI es un sub-campo del aprendizaje de máquina.
:::

::: {.fragment}
- Para algunos, AI y ML sólo es esadística sin preocupaciones. (tiene algo de cierto!)
:::

::: {.fragment}
- En estadística buscamos hacer inferencias (causalidad,) mientras que en AI y ML buscamos predecir (correlación.)
:::

::: {.fragment}
- La AI Generative es un sub-campo de la AI donde el foco es "crear contenido" .
:::
:::
::: {.column width="50%"}
![Lily Popova Zhuhadar, CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, via Wikimedia Commons](fig/Unraveling_AI_Complexity.png)
:::

:::

## ¿Por qué ahora?

::: {.fragment}
- Un componente fundamental: Las Redes Neuronales Artificiales.
:::

::: {.fragment}
- Los modelos de redes neuronales que dan poder a la IA tienen billones de parámetros.
:::

::: {.fragment}
- Más aún, estos modelos adquieren potencial sólo con grandes volúmenes de datos; que sólo se han podido obtener en los últimos años.
:::

::: {.fragment}
- Y por último, la capacidad de procesamiento de los computadores ha aumentado exponencialmente.
:::


## ¿Cómo funcionan estos modelos? {.smaller}

En términos sencillos, **todos** los modelos de IA y aprendizaje de máquina funcionan de la siguiente manera:

::: {.fragment}
1. Obtener datos:

    a. Etiquetados: aprendizaje supervisado.
    b. No etiquetados: aprendizaje no supervisado.
:::
::: {.fragment}
3. Diseñar modelo (arquitectura):

    a. Qué variables de la BD se utilizarán (selección).

    b. Como se procesarán los datos (hyperpárametros).

    c. Definir criterio para aproximar los datos (función de pérdida).
:::
::: {.fragment}
4. Entrenar el modelo  con set de entrenamiento (una parte de los datos).
:::
::: {.fragment}
5. Evaluar la calidad del modelo sobre los datos no usados para entrenar (set de validación.)
:::

## ¿Cuál es el estado del arte? {.smaller}

::: {.fragment}
- El último grito de la moda está con los llamados modelos de lenguaje grandes (*Large Language Models*.)
:::

::: {.fragment}
- Estos se basan en una clase de modelos llamados modelos generativos (*generative models*.)
:::

::: {.fragment}
- La clave: La función de pérdida de estos modelos se centra en predecir sequencias.
:::

::: {.fragment}
- También son implentados con redes neuronales.
:::

## Large language models {.smaller}

NVidia tiene una muy buena descripción de estos modelos [@leeWhatAreLarge2023]:

::: {.fragment}
- **Datos grandes.** Típicamente entrenados en con datos que incluyen casi todo lo que se ha escrito en internet en un largo periodo de tiempo.
:::

::: {.fragment}
- **Redes neuronales.** Se pasa la información a un algorithmo de AI no supervisado.
:::

::: {.fragment}
- **Secuencias + patrones.** Los LLM "aprenden" palabras, relaciones entre ellas, y conceptos. La idea clave: Contexto.
:::

::: {.fragment}
- **Que viene después.** Así como las personas pueden "adivinar" la siguiente palabra (patrones), también los LLM.
:::

# Parte II: Los desafios {background-color="black"}

## Hemos avanzado, pero... {.smaller}


::: {.fragment}
- Con toda la atención que ha recibido la AI/ML, la atención está volviendo a la teoría (inferencia) [@bakerMechanisticModelsMachine2018; @pearlSevenToolsCausal2019]
:::

::: {.fragment}
Revisaremos algunos casos donde la AI/ML ha fallado.
:::


## Caso 1: Google Flu Trends {.smaller}

::: {.fragment}
- Proyecto liderado por Google.org entre el 2008 y 2015 para ayudar a combatir los brotes de influenza.
:::

::: {.fragment}
- La idea era sencilla: el número de búsquedas de palabras asociadas a la influenza está correlacionado con el número de casos de influenza.
:::

::: {.fragment}
- Pero, a pesar de reportar correlaciones cercanas al 100% [@ginsbergDetectingInfluenzaEpidemics2009], llegó a sobreestimar casos en un 100% en EEUU [@kandulaReappraisingUtilityGoogle2019; @lazerParableGoogleFlu2014].
:::


![](fig/google-fail-flu-trends.png){width="90%" fig-align="center"}


## Caso 2: Derechos de Autor {.smaller}

::: {.columns}
::: {.column}

- El diario New York Times [NYT] que ChatGPT reproducia contenido literal del diario.

- El problema es que aquel contenido estaba protegido por un PayWall.

- NYT terminó demandando a OpenAI y Microsoft (dueños de ChatGPT) por violación de derechos de autor [@cnneeDiarioNewYork2023; @munoz-ledoOpenAIAfirmaQue2024].

:::
::: {.column width="50%"}
![Imagen reproducida del artículo @archiveNewYorkTimes2023. ChatGPT fue capaz de extraer información completa de NYT.](https://nypost.com/wp-content/uploads/sites/2/2023/12/new-york-times-sues-openai-74155931.jpg?resize=1024,874&quality=75&strip=all)
:::
:::

## Caso 3: Surreal

- Puede generar imágenes realistas, pero no entiende de anatomía humana:

::: {.columns}

::: {.column width="50%"}

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Midjourney is getting crazy powerful—none of these are real photos, and none of the people in them exist. <a href="https://t.co/XXV6RUrrAv">pic.twitter.com/XXV6RUrrAv</a></p>&mdash; Miles (@mileszim) <a href="https://twitter.com/mileszim/status/1613965684937224192?ref_src=twsrc%5Etfw">January 13, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

:::

::: {.column width="50%"}
![Imagen descargada de @WhyAIgeneratedHands. Dall-E 2 intentando recrear manos humanas.](https://c02.purpledshub.com/uploads/sites/41/2023/02/Dall-e-2-hands-7eba177.jpg?webp=1&w=1200)
:::
:::

## Caso 4: Pinocho {.smaller}

Un correo real que recibí durante el 2023:

> Estimado Sr. Vega Yon,
>
> Soy un académico en Alemania estudiando difusión en redes. Encontré (o mejor dicho, **ChatGPT encontró) que su librería netdiffuseR aparentemente incluía una función llamada "multi.diffusion"** para estimar exactamente ese caso. Parece que había **un artículo** de Wang, Robins y Pattison, "Competing Risks Diffusion in Networks: A Continuation Ratio Model with Time-Varying Effects" **así como un Vignette** llamado "competingrisks" disponible. Sin embargo, **no pude encontrar ninguno de esos.**

::: {.fragment}
¿Cuál creen ustedes que fué el problema?...
:::
::: {.fragment}
¡Dicha función + paper + vignette nunca existieron!
:::

## Caso 5: Demasiado bueno {.smaller}

::: {}
> En resumen, nuestro trabajo suguiere que los datos utilizados para entrenar los mejores LLM pueden ser extraidos con técnicas simples
>
> -- Traducido al español del artículo @nasrScalableExtractionTraining2023
:::

::: {}
Utilizando el texto: "fontanero de video juegos", los autores del artículo lograron extraer la siguiente imagen:
:::

![Imagen descargada de @GenerativeAIHas](fig/plumber.png){width="50%" fig-align="center"}

## {.smaller}

Un texto simple: "captura de pantalla de peliculas populares"

::: {.columns}

::: {.column width="50%"}

![Imagen descargada de @GenerativeAIHas](fig/movies.png){width="70%" fig-align="center"}
:::
::: {.column width="50%"}
- El problema: Como los LLM tienen billones de parámetros, es muy fácil terminar memorizando datos de entrenamiento... eso no es aprendizaje.
:::
:::

# Parte III: La teoría al rescate {background-color="black"}

## Volviendo a lo Básico {.smaller}

::: {.fragment}
- Un motivo por el cual la AI y ML no se han tomado las ciencias por completo es claro (ver ejemplos anteriores).
:::

::: {.fragment}
- El mínimo común de esos problemas: La falta de teoría (no teoría matemática).
:::

::: {.fragment}
- Existe todo un campo en las ciencias de la computación para poder explicar los resultado de la IA/ML.
:::

::: {.fragment}
- Pero con tanto hype, es difícil alejarse de esto.
:::

::: {.fragment}
- Sin embargo, algunos científicos han comenzado a proponer algo distinto: Combinar los modelos mecanísticos con la AI/ML.
:::

## El aprendizaje de máquina mecanístico {.smaller}

El Aprendizaje de Máquina Mechanistico (*mechanistic ML*, o MechML)--también conocido como *theory-guided data science*/*machine learning*: Un híbrido entre modelos guiados por teoría y modelos guiados por los datos.

::: {.columns}

::: {.column width="45%" .fragment}
### Modelos mecanísticos
::: {.callout}
- Centrados en inferencia (causalidad).
- Superiores con pocos datos.
- Encierran información más allá de los datos.
:::
:::
::: {.column width="45%" .fragment}
### Aprendizaje de máquina
::: {.callout}
- Data-driven (predicción).
- Superiores con big data.
- Encuentra patrones "escondidos".
:::
:::

:::

::: {.fragment}
El aprendizaje de máquina ayuda a explicar lo que la teoría no puede... pero aún necesitamos teoría [@lazerParableGoogleFlu2014]!
:::

## Estado del arte en MechML {.smaller}

::: {style="font-size: 70%"}
::: {.fragment}
- Corregir predicciones mechanisticas, como por ejemplo, en modelos basados en agentes [@compagniHybridNeuralNetworkSEIR2022].
:::

::: {.fragment}
- Incorporar información generada con modelos teóricos como una capa de datos adicionales en genética. [@zampieriMachineDeepLearning2019]
:::

::: {.fragment}
- Utilizar rede géneticas ("gene pathways") para incorporar información externa en modelos predictivos. [@altaweraqiImprovedPredictionGene2022]
:::

::: {.fragment}
- Crear funciones de pérdida (en ML) que incorporen penalización mecanística para modelar la densidad de las celulas cancerígenas [@gawIntegrationMachineLearning2019]
:::

::: {.fragment}
- Combinar simulaciones de modelos causales con redes neuronales para predicciones epidemiológicas [@wangTDEFSITheoryguidedDeep2020].
:::

::: {.fragment}
- y mucho más [@jornerMachineLearningMeets2021; @willardIntegratingScientificKnowledge2022a; @jiaPhysicsGuidedMachineLearning2021; @vonruedenInformedMachineLearning2023]
:::
:::

::: {.fragment}
::: {.callout-warning}
1. Mechanistic Machine Learning **no es** diseño de variables. Necesitas un modelo completo mecanístico completo para integrar con el modelo predictivo.

2. Tampoco es lo que se conoce como ML ensamble, donde se mezclan predicciones después de generalas.
:::

:::

## Caso 1: Detectando la temperatura de la superficie del agua {style="font-size: 60%"}

- En @jiaPhysicsGuidedMachineLearning2021, los autores presentan su modelo de "redes neuronales recurrentes guiadas por la física" (*physics-guided recurrent neural network model*, o PGRNN)"

::: {.columns}
::: {.column width="50%"}
![](fig/pgrnn.png)
:::
::: {.column width="50%"}
![](fig/lake-model.png)
:::
:::

> Así, PGRNN puede aprovechar las fortalezas de los modelos basados en la física y llena vacíos de conocimiento empleando modelos predictivos de vanguardia que aprenden de los datos. -- @jiaPhysicsGuidedMachineLearning2021

## Caso 2: Mejorando los pronósticos de la influenza {style="font-size: 60%"}

- @wangTDEFSITheoryguidedDeep2020 presenta el modelo de predicción epidemiológica utilizando redes neuronales guiadas por modelos teóricos (*Theory-guided Deep Learning-based Epidemic Forecasting with Synthetic Information*, o TDEFSI).

::: {layout-align="center"}
![](fig/theory-guided-influenza-forecast.png){width=80%}
:::

## Caso 3: Prediciendo lo que hacen los genes {style="font-size: 60%"}

- Junto con colaboradores de USC, desarrollamos un modelo de MechML para predecir funciones genéticas (en proceso de publicación).

::: {layout-align="center"}
![](fig/aphylo-mechml.png){width="80%"}
:::

- En un experimento, el error de predicción bajó de 0.29 (mecanístico) a 0.14.


## Discusión {.smaller}

::: {.fragment}
- La AI/ML tiene una capacidad infinita de potenciar la productividad humana.
:::

::: {.fragment}
- LA AI generativa explota grandes volúmenes de datos para identificar patrones (secuencias).
:::

::: {.fragment}
- Pero, la AI/ML no es perfecta... todavía necesitamos la teoría!
:::

::: {.fragment}
- El aprendizaje de máquina mecanístico (*Mechanistic ML*) es un híbrido entre modelos guiados por teoría y modelos guiados por los datos.
:::

::: {.fragment}
- El MechML ha demostrado tener la capacidad de mejorar la predicción de modelos de AI/ML y teóricos.
:::

::: {.fragment}
- Inyectar teoría en la AI/ML es el siguiente paso en la revolución de la AI.
:::

## Fin {background-color="black"}

::: {style="text-align: center"} 
¡Gracias!

<text style="font-weight: bold; font-size: 120%">"La Muerte del Aprendizaje de Máquina"</text>

George G. Vega Yon, Ph.D.

[https://ggv.cl](https://ggv.cl)

george.vegayon@utah.edu

:::

## Referencias